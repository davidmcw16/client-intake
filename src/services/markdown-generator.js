const Anthropic = require('@anthropic-ai/sdk');

const client = new Anthropic({ apiKey: process.env.LLM_API_KEY });

const GENERATION_PROMPT = `You are a technical writer. Given a conversation transcript between an AI interviewer and a client, generate a structured project intake document in the exact markdown format specified below.

Synthesize the information — don't just copy/paste raw answers. Clean up spoken language into clear written descriptions. Organize scattered information logically. Fill in implied details.

OUTPUT FORMAT (follow exactly):

# Project Intake: [Client Name] — [Date]

> Voice intake completed in [X] minutes ([Y] conversation turns)

---

## FEATURE

### Vision
[Synthesized from the full conversation — what they want to build, in clear prose]

### Problem & Users
[Who uses it, what problem it solves, why it matters — synthesized, not just pasted]

### Core Capabilities
[The must-have features, organized and described clearly]

### User Journey
[Step-by-step walkthrough of a typical user interaction]

---

## DESIGN & EXPERIENCE

### Look & Feel
[Visual style, mood, personality — everything design-related from the conversation]

---

## DEPENDENCIES & INTEGRATIONS

### External Systems
[Tools, services, platforms it connects to — or "None identified" if not applicable]

### Scale & Performance
[Expected users, growth expectations]

---

## OTHER CONSIDERATIONS
[Timeline, budget, platform preferences, dealbreakers, anything else mentioned]

---

## Full Conversation Transcript

[Include the full transcript here, formatted as:]
[AI]: message
[Client]: message

---

*Generated by Voice Intake — [Date]*`;

/**
 * Generate a structured markdown brief from a conversation transcript.
 *
 * @param {Array<{role: string, message: string}>} transcript - From ElevenLabs webhook
 * @param {string} clientName - Extracted by ElevenLabs analysis
 * @param {{durationSeconds: number, turnCount: number}} metadata - Conversation metadata
 * @returns {Promise<string>} Generated markdown
 */
async function generateMarkdown(transcript, clientName, metadata) {
  const formattedTranscript = transcript
    .map(t => `[${t.role === 'agent' ? 'AI' : 'Client'}]: ${t.message}`)
    .join('\n');

  const durationMinutes = Math.round((metadata.durationSeconds || 0) / 60);

  const userMessage = `Client name: ${clientName || 'Client'}
Date: ${new Date().toISOString().split('T')[0]}
Duration: ${durationMinutes} minutes
Turns: ${metadata.turnCount || 0}

CONVERSATION TRANSCRIPT:
${formattedTranscript}`;

  const model = process.env.LLM_MODEL || 'claude-sonnet-4-5-20250929';

  const response = await client.messages.create({
    model,
    max_tokens: 4096,
    system: GENERATION_PROMPT,
    messages: [{ role: 'user', content: userMessage }],
  });

  return response.content[0].text;
}

/**
 * Generate a fallback markdown brief when Claude API is unavailable.
 * Contains just the raw transcript — no synthesis.
 *
 * @param {Array<{role: string, message: string}>} transcript
 * @param {string} clientName
 * @param {{durationSeconds: number, turnCount: number}} metadata
 * @returns {string} Fallback markdown
 */
function generateFallback(transcript, clientName, metadata) {
  const date = new Date().toISOString().split('T')[0];
  const durationMinutes = Math.round((metadata.durationSeconds || 0) / 60);

  const formattedTranscript = transcript
    .map(t => `[${t.role === 'agent' ? 'AI' : 'Client'}]: ${t.message}`)
    .join('\n');

  return `# Project Intake: ${clientName || 'Client'} — ${date}

> Voice intake completed in ${durationMinutes} minutes (${metadata.turnCount || 0} conversation turns)
> **Note:** Automated brief generation failed. Manual synthesis required.

---

## Full Conversation Transcript

${formattedTranscript}

---

*Generated by Voice Intake — ${date}*
*Automated brief generation was unavailable. Raw transcript preserved.*`;
}

module.exports = { generateMarkdown, generateFallback };
